{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 单热编码\n",
    "\n",
    "在第一部分，我们从零起步，帮助小明创建了一个预测冰激淋销量的**多层神经网络**；在第二部分，我们亲手搭建了一个**神经网络训练框架**，支持全连接网络，解决了数值回归任务；在第三部分，我们进一步扩展了框架，使其具备了支持分类任务，处理多维度数据，和局部特征提取（CNN）的能力。\n",
    "\n",
    "到目前为止，我们还停留在处理静态数据、“瞬时场景”的阶段。模型处理的每一组销量数据，识别的每一张图片，都是彼此独立的个体。\n",
    "\n",
    "而另一类数据，我们称之为**序列数据**（Sequence Data）。这一类数据，我们不能只看当前的一组数据，还要参考之前的所有数据。比如股价，我们不能只看今天的价格，还要参考之前很长时间的股价变化来做分析。\n",
    "\n",
    "在所有序列数据中，最贴近人类智能、也最复杂的，莫过于语言。\n",
    "\n",
    "这一部分，我们将继续扩充我们的神经网络训练框架，尝试处理文字信息。\n",
    "\n",
    "---\n",
    "\n",
    "我们需要面对的第一个问题是：如何数字化文字信息。\n",
    "\n",
    "神经网络只能处理数字信息，我们需要一种技术将文字转换成数字。\n",
    "\n",
    "一种最直接的方法是：把所有单词排列起来，分别给它们一个编号，称为**索引编码**（Index-Based Encoding）。\n",
    "\n",
    "比如：假设排在前三位的单词分别是“你”、“我”、“他”，那么它们的编号就分别是 0、1、2。如果我们采用 GB-2312 汉字库，一共有 6763 个汉字，可以分别编号为 0 到 6762。\n",
    "\n",
    "但是**索引编码**仍然不适用于神经网络，因为可能会让模型认为 1 代表的“我”大于 0 代表的“你”。\n",
    "\n",
    "---\n",
    "\n",
    "我们在上一部分的 MNIST 数据集中使用过的**单热编码**（One-Hot Encoding）正好可以派上用场。\n",
    "\n",
    "**单热编码**的核心思想非常直观：它将每一个类别映射为一个向量，向量的长度和类别的总数相同。在这个向量中，只有一个位置的值是 1，而其余所有位置的值都是 0。\n",
    "\n",
    "还以 GB-2312 汉字库为例。一共 6763 个汉字，所以每个汉字将被映射到一个长度为 6763 的向量。“你”的编码只有第 1 个位置是 1，“我”的编码只有第 2 个位置是 1。\n",
    "\n",
    "|  词汇  |  索引编码  |         单热编码向量         |\n",
    "|:----:|:------:|:----------------------:|\n",
    "|  你   |   0    |  [1, 0, 0, 0, 0, ...]  |\n",
    "|  我   |   1    |  [0, 1, 0, 0, 0, ...]  |\n",
    "|  他   |   2    |  [0, 0, 1, 0, 0, ...]  |"
   ],
   "id": "9e3b77c72c43376c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-26T22:02:37.082657Z",
     "start_time": "2026-01-26T22:02:37.074086Z"
    }
   },
   "source": [
    "import csv\n",
    "import re\n",
    "from abc import abstractmethod, ABC\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基础架构",
   "id": "d2807c9c7c24ade7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量",
   "id": "76ffa09456a4b1d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:02:37.096085Z",
     "start_time": "2026-01-26T22:02:37.090167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.backward()\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.data})'"
   ],
   "id": "98da1e7f4883cb6e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础数据集",
   "id": "808f34399fedc7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:02:37.101727Z",
     "start_time": "2026-01-26T22:02:37.096853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset(ABC):\n",
    "\n",
    "    def __init__(self, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.load()\n",
    "        self.train()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        self.features = self.train_features\n",
    "        self.labels = self.train_labels\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = self.test_features\n",
    "        self.labels = self.test_labels\n",
    "\n",
    "    def shape(self):\n",
    "        return Tensor(self.features).size, Tensor(self.labels).size\n",
    "\n",
    "    def items(self):\n",
    "        return Tensor(self.features), Tensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "\n",
    "        feature = Tensor(self.features[start: end])\n",
    "        label = Tensor(self.labels[start: end])\n",
    "        return feature, label\n",
    "\n",
    "    def estimate(self, predictions):\n",
    "        pass"
   ],
   "id": "3184c7ebd35741e4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "4c2e8dcfac6097b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### IMDB 数据集\n",
    "\n",
    "在这一部分，我们将使用来自 IMDB 的影评数据集。当然，为了加快模型训练速度，我们依旧制作了一个迷你版本：TinyIMDB。\n",
    "\n",
    "IMDB 数据集的每个样本包括两个数据：\n",
    "\n",
    "* **观众影评**：这是一段由观众书写的文字信息；\n",
    "* **观众态度**：观众给出的评价（“喜欢”， 或者“讨厌”）。\n",
    "\n",
    "---\n",
    "\n",
    "在加载 IMDB 数据集时，我们将对数据进行预处理：\n",
    "\n",
    "* **数据清洗**：数据清洗的目的在于去除影评中的 HTML 标签和标点符号；\n",
    "* **构建词表**：统计数据集用到的所有单词，汇总成单词列表；\n",
    "* **索引编码**：对单词列表进行索引编码；\n",
    "\n",
    "在 IMDB 数据集里，我们还提供了几个和编码有关的函数：\n",
    "\n",
    "* **编码函数**（encode）：返回单词的索引编码；\n",
    "* **解码函数**（decode）：返回索引编码对应的单词；\n",
    "* **单热编码函数**（onehot）：返回索引编码的单热编码；\n",
    "* **参数最大化函数**（argmax）：根据单热编码返回索引编码。"
   ],
   "id": "9091f1a11751a31f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:02:37.109791Z",
     "start_time": "2026-01-26T22:02:37.102147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMDBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        super().__init__()\n",
    "\n",
    "    def load(self):\n",
    "        self.reviews = []\n",
    "        self.sentiments = []\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for _, row in enumerate(reader):\n",
    "                self.reviews.append(row[0])\n",
    "                self.sentiments.append(row[1])\n",
    "\n",
    "        split_reviews = []\n",
    "        for line in self.reviews:\n",
    "            split_reviews.append(self.clean_text(line.lower()).split())\n",
    "\n",
    "        self.vocabulary = set(word for line in split_reviews for word in line)\n",
    "        self.word2index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.index2word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.tokens = [[self.word2index[word] for word in line if word in self.word2index] for line in split_reviews]\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def train(self):\n",
    "        self.features = [list(set(index)) for index in self.tokens[:-10]]\n",
    "        self.labels = [0 if index == \"negative\" else 1 for index in self.sentiments[:-10]]\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = [list(set(index)) for index in self.tokens[-10:]]\n",
    "        self.labels = [0 if index == \"negative\" else 1 for index in self.sentiments[-10:]]\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.clean_text(text.lower()).split()\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([self.index2word[index] for index in tokens])\n",
    "\n",
    "    def onehot(self, token):\n",
    "        ebd = np.zeros(len(self.vocabulary))\n",
    "        ebd[token] = 1\n",
    "        return ebd\n",
    "\n",
    "    @staticmethod\n",
    "    def argmax(vector):\n",
    "        return [np.argmax(vector)]"
   ],
   "id": "43f534698d141456",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "87aebc33b105f956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 测试",
   "id": "4e46ee55186c7c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T22:02:37.126241Z",
     "start_time": "2026-01-26T22:02:37.110340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = IMDBDataset('tinyimdb.csv')\n",
    "print('Vocabulary count:', len(dataset.vocabulary))\n",
    "print('Review: ', dataset.reviews[0])\n",
    "print('Tokens: ', dataset.tokens[0])\n",
    "print('Sentiment: ', dataset.sentiments[0])\n",
    "\n",
    "message = 'i recommend this film'\n",
    "print('Message: ', message)\n",
    "\n",
    "tokens = dataset.encode(message)\n",
    "print('Encode: ', tokens)\n",
    "print('Decode: ', dataset.decode(tokens))\n",
    "print('Onehot: ', dataset.onehot(tokens[0]))"
   ],
   "id": "661830cf3be02a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary count: 86\n",
      "Review:  this movie was excellent. i enjoyed the plot and acting. the character was wonderful. recommend. screenplay actor actress by is a\n",
      "Tokens:  [7, 84, 14, 64, 43, 21, 1, 0, 51, 71, 1, 29, 14, 83, 34, 13, 8, 46, 35, 69, 75]\n",
      "Sentiment:  positive\n",
      "Message:  i recommend this film\n",
      "Encode:  [43, 34, 7, 2]\n",
      "Decode:  i recommend this film\n",
      "Onehot:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
