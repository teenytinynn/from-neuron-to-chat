{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 词袋嵌入\n",
    "\n",
    "我们通过**索引编码**把文字转换成数字，通过**单热编码**把数字转换成分类，从而解决了用**文字数据集**训练网络模型的第一步。\n",
    "\n",
    "我们需要面对的第二个问题是：如何处理文字的**序列**（Sequence）。\n",
    "\n",
    "文字是一种序列数据，我们不但要关心当前单词本身，还要考虑**上下文**（临近其他文字）的影响。所以我们不能单独研究一个单词，而是需要同时研究一句话，一段话，甚至一整篇文章。\n",
    "\n",
    "一个直觉的方法是把一句话的索引编码作为一个向量来使用。\n",
    "\n",
    "比如：可以把“我去上学”这句话的索引编码向量 [0, 8, 23, 48] 当作输入数据。这个向量不仅包括了一句话使用的单词，还包括每个单词的位置信息。\n",
    "\n",
    "但是也有人会说：“我上学去”。索引编码向量是 [0, 23, 48, 8]。\n",
    "\n",
    "这两句话意思相同，但是两个索引编码向量却只有 25% 相同。原因就是这种方法使用了文字的**绝对位置**信息。如果我们把使用**绝对位置**的向量，改成使用**相邻关系**的集合呢？那么这两句话的索引编码集合就都是 (0, 8, 23, 48)。\n",
    "\n",
    "这种表达语序的方法，称为**词袋**（Bag of Words）。它的核心思路也很简单：把一句话使用到的单词，不分先后、不计重复，放到一个集合中。\n",
    "\n",
    "---\n",
    "\n",
    "“我去上学”和“我上学去”这两句话的词袋是 (0, 8, 23, 48)。如果词表长度是 50 的话，对应的单热编码矩阵就是：\n",
    "\n",
    "```data\n",
    "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "  0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n",
    "```\n",
    "\n",
    "这还是词表长度只有 50 的情况。可以想象如果词表长度是数千、甚至数万的话，一句话的单热编码会是多么巨大。\n",
    "\n",
    "对于单热编码，词袋有一种有效的压缩方式，就是把所有单热编码向量合并在一起。因为每个单词的单热编码都占据一个不同的位置，所以合并在一起并不会互相遮盖。比如上面的这个单热编码矩阵就可以压缩成一个向量：\n",
    "\n",
    "```data\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    " 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "对于单热编码这一类稀疏向量，另一种压缩方法是：**嵌入**（Embedding）。\n",
    "\n",
    "它的核心思想并不复杂：每个单热编码向量只使用一个位置（或者合并后使用少量位置），其他都是 0。这种表达方式用于分类问题的标签值是有意义的；但是用于特征值却是巨大的浪费，同时会造成权重参数“爆表”。那么是否可以参考数据压缩技术，把每个单热编码向量压缩到一个相对较短的向量？\n",
    "\n",
    "比如：把上面长度为 50 的单热编码向量压缩到长度为 8 或者 16 的向量；或者，把实际应用中长度几千，甚至几万的单热编码向量压缩到长度为 128 或者 256 的向量。\n",
    "\n",
    "这一章节，我们将尝试采用网络模型“自学习”的方式，达到这种”嵌入“的效果。"
   ],
   "id": "9e3b77c72c43376c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:32.983095Z",
     "start_time": "2026-01-31T16:39:32.971682Z"
    }
   },
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "from abc import abstractmethod, ABC\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "outputs": [],
   "execution_count": 583
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基础架构",
   "id": "4ad3e91e8f5dd61f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量",
   "id": "76ffa09456a4b1d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.012481Z",
     "start_time": "2026-01-31T16:39:32.995858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.backward()\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.data})'"
   ],
   "id": "98da1e7f4883cb6e",
   "outputs": [],
   "execution_count": 584
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础数据集",
   "id": "808f34399fedc7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.027791Z",
     "start_time": "2026-01-31T16:39:33.014130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset(ABC):\n",
    "\n",
    "    def __init__(self, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.test_labels = self.test_features = None\n",
    "        self.train_labels = self.train_features = None\n",
    "\n",
    "        self.load()\n",
    "        self.train()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        self.features = self.train_features\n",
    "        self.labels = self.train_labels\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = self.test_features\n",
    "        self.labels = self.test_labels\n",
    "\n",
    "    def shape(self):\n",
    "        return Tensor(self.features).size, Tensor(self.labels).size\n",
    "\n",
    "    def items(self):\n",
    "        return Tensor(self.features), Tensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "\n",
    "        feature = Tensor(self.features[start: end])\n",
    "        label = Tensor(self.labels[start: end])\n",
    "        return feature, label\n",
    "\n",
    "    def estimate(self, predictions):\n",
    "        pass"
   ],
   "id": "3184c7ebd35741e4",
   "outputs": [],
   "execution_count": 585
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础层",
   "id": "9630f69ccfc5eadd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.038532Z",
     "start_time": "2026-01-31T16:39:33.029263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ''"
   ],
   "id": "c9430289f62a18cf",
   "outputs": [],
   "execution_count": 586
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础损失函数",
   "id": "a7d76d1f238e2f2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.043480Z",
     "start_time": "2026-01-31T16:39:33.038910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(ABC):\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        return self.loss(p, y)\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        pass"
   ],
   "id": "3c54354f9cc55f52",
   "outputs": [],
   "execution_count": 587
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础优化器",
   "id": "440d33480b751ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.047570Z",
     "start_time": "2026-01-31T16:39:33.043820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer(ABC):\n",
    "\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass"
   ],
   "id": "ca36d13a92575aee",
   "outputs": [],
   "execution_count": 588
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础模型",
   "id": "cba0d133a47f403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.052279Z",
     "start_time": "2026-01-31T16:39:33.047902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(ABC):\n",
    "\n",
    "    def __init__(self, layer, loss_fn, optimizer):\n",
    "        self.layer = layer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, dataset, epochs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, dataset):\n",
    "        pass"
   ],
   "id": "9f11e2dd6492be7c",
   "outputs": [],
   "execution_count": 589
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "3e3bc5aaf4aedca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### IMDB 数据集\n",
    "\n",
    "在这个章节，我们将训练一个网络模型，可以根据观众影评的内容来预测观众态度。\n",
    "\n",
    "我们把所有观众影评和观众态度分为训练集（90%）和测试集（10%）。\n",
    "\n",
    "* **观众影评**：我们用**词袋**把每条观众影评从一段文字转化成一个索引编码的集合（无次序、无重复）。\n",
    "\n",
    "* **观众态度**：从文字描述转化成数字类别，只有两种：\n",
    "    * **1**：代表“喜欢”；\n",
    "    * **0**：代表”讨厌“。\n",
    "\n",
    "网络模型的预测结果将是 [0, 1] 区间内的小数：\n",
    "\n",
    "* **> 0.5**：预测观众态度是”喜欢“；\n",
    "* **< 0.5**：预测观众态度是“讨厌”。\n",
    "\n",
    "我们还实现了基础数据集的**评估函数**（estimate），来评估网络模型预测的准确率。"
   ],
   "id": "48a0c19bdc0c9ac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.059788Z",
     "start_time": "2026-01-31T16:39:33.052654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMDBDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        super().__init__()\n",
    "\n",
    "    def load(self):\n",
    "        self.reviews = []\n",
    "        self.sentiments = []\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for _, row in enumerate(reader):\n",
    "                self.reviews.append(row[0])\n",
    "                self.sentiments.append(row[1])\n",
    "\n",
    "        split_reviews = []\n",
    "        for line in self.reviews:\n",
    "            split_reviews.append(self.clean_text(line.lower()).split())\n",
    "\n",
    "        self.vocabulary = set(word for line in split_reviews for word in line)\n",
    "        self.word2index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.index2word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.tokens = [[self.word2index[word] for word in line if word in self.word2index] for line in split_reviews]\n",
    "\n",
    "        # 词袋\n",
    "        self.train_features = [list(set(index)) for index in self.tokens[:-10]]\n",
    "        self.test_features = [list(set(index)) for index in self.tokens[-10:]]\n",
    "        # 标签\n",
    "        self.train_labels = [0 if index == \"negative\" else 1 for index in self.sentiments[:-10]]\n",
    "        self.test_labels = [0 if index == \"negative\" else 1 for index in self.sentiments[-10:]]\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.clean_text(text.lower()).split()\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([self.index2word[index] for index in tokens])\n",
    "\n",
    "    def onehot(self, token):\n",
    "        ebd = np.zeros(len(self.vocabulary))\n",
    "        ebd[token] = 1\n",
    "        return ebd\n",
    "\n",
    "    @staticmethod\n",
    "    def argmax(vector):\n",
    "        return np.argmax(vector)\n",
    "\n",
    "    def estimate(self, predictions):\n",
    "        count = 0\n",
    "        for i in range(len(predictions)):\n",
    "            if np.abs(predictions[i].data - self.labels[i]) < 0.5:\n",
    "                count += 1\n",
    "        return count / len(predictions)"
   ],
   "id": "94e64b4c0eabcaff",
   "outputs": [],
   "execution_count": 590
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "a7ec8d93ec84d28e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 线性层",
   "id": "615461506c3e6e7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.064734Z",
     "start_time": "2026-01-31T16:39:33.060169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.weight = Tensor(np.random.randn(out_size, in_size) * np.sqrt(2 / in_size))\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad += p.grad.T @ x.data\n",
    "            self.bias.grad += np.sum(p.grad, axis=0)\n",
    "            x.grad += p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear[weight{self.weight.data.shape}; bias{self.bias.data.shape}]'"
   ],
   "id": "36ee847b469bb5ff",
   "outputs": [],
   "execution_count": 591
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 顺序层",
   "id": "dd3f9a88069f81c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.071329Z",
     "start_time": "2026-01-31T16:39:33.065049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def train(self):\n",
    "        for l in self.layers:\n",
    "            l.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for l in self.layers:\n",
    "            l.eval()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join(str(l) for l in self.layers if str(l))"
   ],
   "id": "392c3f8e528297c9",
   "outputs": [],
   "execution_count": 592
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 嵌入层\n",
    "\n",
    "**嵌入层**（Embedding Layer）的目的是将巨大，但是稀疏的单热编码向量压缩到一个比较小的尺寸。\n",
    "\n",
    "嵌入层的基本功能和现行层是很相似的，都是将输入向量使用权重加工成输出向量。但是嵌入层的输入数据是独特的单热编码向量，每个向量有且只有一个位置是 1，其他位置都是 0。所以嵌入层直接使用输入单热编码向量来过滤权重，不用复杂的矢量乘法，快速地得到输出数据。\n",
    "\n",
    "举个例子：如果我们的词表一共有 10000 个单词，当前的输入数据是一段 25 个单词的观众影评。那么输入数据的格式就是 25 个单热编码向量，数据格式为 (25, 10000)。我们希望将单热编码向量的长度压缩到 64。\n",
    "\n",
    "如果对比**线性层**的话，嵌入层的**输入特征维度**（in_size）就是 10000，而**输出特征维度**（out_size）就是 64。而输入数据的**批大小**（batch_size）是 25。\n",
    "\n",
    "嵌入层将创建一个规格为 (64, 10000) 的权重矩阵，相当于词表中每个单词都有一个对应的权重向量。嵌入层的计算过程，就是用单热编码向量来截取权重对应位置的权重向量。因此我们的计算结果就是一个（转置后）规格为 (25, 64) 的权重子集。\n",
    "\n",
    "嵌入层的对单热编码的压缩结果，实际上就是单热编码对应位置上的权重。\n",
    "\n",
    "最终，嵌入层把计算结果求平均，合并成一个输出向量。无论一句话有多少单词，作为一个词袋，最后只会输出一个向量。"
   ],
   "id": "6c9c5a3db3e716e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.081873Z",
     "start_time": "2026-01-31T16:39:33.074818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # 词表中的每个单词，都有自己的权重\n",
    "        self.weight = Tensor(np.random.randn(embedding_size, vocabulary_size) * np.sqrt(2 / vocabulary_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.mean(self.weight.data.T[x.data], axis=1))\n",
    "\n",
    "        def gradient_fn():\n",
    "            if type(self.weight.grad) is not np.ndarray:\n",
    "                self.weight.grad = np.zeros_like(self.weight.data)\n",
    "            self.weight.grad.T[x.data] += p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight}\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Embedding[weight{self.weight.data.shape}; vocabulary={self.vocabulary_size}; embedding={self.embedding_size}]'"
   ],
   "id": "60aaa96958f5dac7",
   "outputs": [],
   "execution_count": 593
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ReLU 激活函数",
   "id": "8b160d6f6ddf2e40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.087882Z",
     "start_time": "2026-01-31T16:39:33.082586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.maximum(0, x.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad += p.grad * (p.data > 0)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ReLU[]'"
   ],
   "id": "56103804f52a80e5",
   "outputs": [],
   "execution_count": 594
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sigmoid 激活函数",
   "id": "10446f336ec60817"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.092878Z",
     "start_time": "2026-01-31T16:39:33.088255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "    def __init__(self, clip_range=(-100, 100)):\n",
    "        super().__init__()\n",
    "        self.clip_range = clip_range\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        z = np.clip(x.data, self.clip_range[0], self.clip_range[1])\n",
    "        p = Tensor(1 / (1 + np.exp(-z)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad += p.grad * p.data * (1 - p.data)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Sigmoid[]'"
   ],
   "id": "4d3c53da9f19f4be",
   "outputs": [],
   "execution_count": 595
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 损失函数（二元交叉熵）",
   "id": "fd6decedde0cfbfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.099492Z",
     "start_time": "2026-01-31T16:39:33.093197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BCELoss(Loss):\n",
    "\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        clipped = np.clip(p.data, 1e-7, 1 - 1e-7)\n",
    "        bce = Tensor(-np.mean(y.data * np.log(clipped) + (1 - y.data) * np.log(1 - clipped)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad += (clipped - y.data) / (clipped * (1 - clipped)) / len(y.data)\n",
    "\n",
    "        bce.gradient_fn = gradient_fn\n",
    "        bce.parents = {p}\n",
    "        return bce"
   ],
   "id": "648d5527ecd611e6",
   "outputs": [],
   "execution_count": 596
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 优化器（随机梯度下降）",
   "id": "3f0096e38462b2f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.104033Z",
     "start_time": "2026-01-31T16:39:33.099928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDOptimizer(Optimizer):\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad * self.lr"
   ],
   "id": "e9a556872b1df86",
   "outputs": [],
   "execution_count": 597
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 神经网络模型\n",
    "\n",
    "我们对模型类也做了一点调整。\n",
    "\n",
    "就是在**测试函数**（test）里使用了一个循环，逐条测试。这是因为每条观众影评的长度都不一样，不能直接使用批处理。"
   ],
   "id": "e45bc59a4ddb16a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.108534Z",
     "start_time": "2026-01-31T16:39:33.104317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WBModel(Model):\n",
    "\n",
    "    def train(self, dataset, epochs):\n",
    "        self.layer.train()\n",
    "        dataset.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(dataset)):\n",
    "                features, labels = dataset[i]\n",
    "\n",
    "                predictions = self.layer(features)\n",
    "                loss = self.loss_fn(predictions, labels)\n",
    "                self.optimizer.reset()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def test(self, dataset):\n",
    "        self.layer.eval()\n",
    "        dataset.eval()\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(len(dataset)):\n",
    "            features, labels = dataset[i]\n",
    "            predictions.append(self.layer(features))\n",
    "        return predictions"
   ],
   "id": "82558b95c4a1d5be",
   "outputs": [],
   "execution_count": 598
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 设置",
   "id": "a0dc95f4aa75cc14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 学习率",
   "id": "eb496e2365188f53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.112037Z",
     "start_time": "2026-01-31T16:39:33.108869Z"
    }
   },
   "cell_type": "code",
   "source": "LEARNING_RATE = 0.01",
   "id": "725036a072c743a",
   "outputs": [],
   "execution_count": 599
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 轮次",
   "id": "37c577e2f3d1fa2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.115933Z",
     "start_time": "2026-01-31T16:39:33.112259Z"
    }
   },
   "cell_type": "code",
   "source": "EPOCHS = 100",
   "id": "9db52ae94644c258",
   "outputs": [],
   "execution_count": 600
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "631dcfc126823cd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 迭代\n",
    "\n",
    "我们在**顺序层**里，首先使用**嵌入层**作为第一个子层，将顾客影评转换成 32 位的向量，然后连接两层线性层作为隐藏层。\n",
    "\n",
    "在输出层，我们使用了 Sigmoid 激活函数，因为这是一个二元分类问题。"
   ],
   "id": "182c437e48259ef2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.774283Z",
     "start_time": "2026-01-31T16:39:33.116316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = IMDBDataset('tinyimdb.csv')\n",
    "layer = Sequential([Embedding(len(dataset.vocabulary), 32),\n",
    "                    ReLU(),\n",
    "                    Linear(32, 8),\n",
    "                    ReLU(),\n",
    "                    Linear(8, 1),\n",
    "                    Sigmoid()])\n",
    "loss = BCELoss()\n",
    "optimizer = SGDOptimizer(layer.parameters, lr=LEARNING_RATE)\n",
    "\n",
    "model = WBModel(layer, loss, optimizer)\n",
    "model.train(dataset, EPOCHS)\n",
    "print(layer)"
   ],
   "id": "157fe17f92697e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding[weight(32, 86); vocabulary=86; embedding=32]\n",
      "ReLU[]\n",
      "Linear[weight(8, 32); bias(8,)]\n",
      "ReLU[]\n",
      "Linear[weight(1, 8); bias(1,)]\n",
      "Sigmoid[]\n"
     ]
    }
   ],
   "execution_count": 601
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "8ac58a2e86c018fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 测试\n",
    "\n",
    "**词袋**主要关注在输入数据中，某个单词是否处想，而忽略掉单词的位置信息。\n",
    "\n",
    "这种简单策略已经可以有效地处理分类判断问题，比如：鉴别垃圾邮件、分类客户评论等。"
   ],
   "id": "4e46ee55186c7c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.788765Z",
     "start_time": "2026-01-31T16:39:33.775422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = model.test(dataset)\n",
    "print(f'Accuracy: {dataset.estimate(predictions)}')"
   ],
   "id": "661830cf3be02a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 602
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 近义词\n",
    "\n",
    "最后，让我们来看一个**嵌入层**神奇、有趣的附带结果：近义词。\n",
    "\n",
    "我们已经知道，嵌入层的输出结果，实际上就是输入数据的单热编码对应的权重。\n",
    "\n",
    "我们想象一下，如果有两个单词时近义词，甚至是同义词。那么在文章里，它们就会经常出现在相同的位置。因此，经过模型训练以后，它们对应的嵌入层权重大概率会被更新到很相似的状态。\n",
    "\n",
    "如果我们选择一个单词，然后对比它和所有其他单词和权重。就会发现和它的权重相比，损失值最小的基本上都是它的近义词。\n",
    "\n",
    "我们来验证一下。"
   ],
   "id": "de43045ed45c2b50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T16:39:33.818200Z",
     "start_time": "2026-01-31T16:39:33.794192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def similar(dataset, layer, target='excellent'):\n",
    "    target_index = dataset.word2index[target]\n",
    "    scores = {}\n",
    "\n",
    "    for word, index in dataset.word2index.items():\n",
    "        raw_diff = layer.layers[0].weight.data.T[index] - layer.layers[0].weight.data.T[target_index]\n",
    "        squared_diff = raw_diff ** 2\n",
    "        scores[word] = math.sqrt(sum(squared_diff))\n",
    "\n",
    "    return dict(sorted(scores.items(), key=lambda i: i[1])[:10])\n",
    "\n",
    "print(similar(dataset, layer, target='excellent'))\n",
    "print(similar(dataset, layer, target='terrible'))"
   ],
   "id": "ed606e2a5e2eaaf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'excellent': 0.0, 'fantastic': 1.1482987382812186, 'enjoyed': 1.1633913355021896, 'perfect': 1.3418945594281766, 'great': 1.4629123299682554, 'best': 1.4768419809621178, 'amazing': 1.515310402469664, 'cinematography': 1.5633716277447003, 'effect': 1.6361294801935369, 'loved': 1.6735511363799018}\n",
      "{'terrible': 0.0, 'awful': 1.0119600682370666, 'hated': 1.0318255812574781, 'boring': 1.0867870174258742, 'poor': 1.3439985427434429, 'horrible': 2.5289584274507635, 'mine': 2.6468342076971245, 'of': 2.7000707721715864, 'disappointed': 2.8908036137952786, 'action': 3.034170881959871}\n"
     ]
    }
   ],
   "execution_count": 603
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们的数据集很小，但是依旧非常成功地鉴别出近义词。",
   "id": "67b22cf03d4b594e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 课后练习\n",
    "\n",
    "试一试，能不能找出反义词？"
   ],
   "id": "52821f4cd474ec48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
