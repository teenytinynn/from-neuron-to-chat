{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 激活函数层\n",
    "\n",
    "激活函数是人工神经元的一部分。但是在深度学习的实践中，通常都是被设计成单独的一层，和线性层配合使用。"
   ],
   "id": "c0dcf040878af3b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.579534385Z",
     "start_time": "2026-01-19T12:12:54.534293334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "id": "f2f5f22af2a86f3e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基础架构",
   "id": "602af6c9b9ac7c6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 张量",
   "id": "3dcda0116d9179b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.617206330Z",
     "start_time": "2026-01-19T12:12:54.591008725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.backward()\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self.data.shape[-1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.data})'"
   ],
   "id": "c73ebea589dd19d8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础数据集",
   "id": "e29266672fff094f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.650433644Z",
     "start_time": "2026-01-19T12:12:54.621258659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset(ABC):\n",
    "\n",
    "    def __init__(self, batch_size=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.load()\n",
    "        self.train()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        self.features = self.train_features\n",
    "        self.labels = self.train_labels\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = self.test_features\n",
    "        self.labels = self.test_labels\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return Tensor(self.features).size, Tensor(self.labels).size\n",
    "\n",
    "    def items(self):\n",
    "        return Tensor(self.features), Tensor(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "\n",
    "        feature = Tensor(self.features[start: end])\n",
    "        label = Tensor(self.labels[start: end])\n",
    "        return feature, label"
   ],
   "id": "fde9ac4c2e187db5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础层",
   "id": "3e4acfae657d6646"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.683614421Z",
     "start_time": "2026-01-19T12:12:54.656285367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.forward(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ''"
   ],
   "id": "da65a5787200d264",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础损失函数",
   "id": "84802851c1fc1134"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.712740238Z",
     "start_time": "2026-01-19T12:12:54.689868920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(ABC):\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        return self.loss(p, y)\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        pass"
   ],
   "id": "69b74596c9d28e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础优化器",
   "id": "93d5952827155637"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.737978615Z",
     "start_time": "2026-01-19T12:12:54.716977436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer(ABC):\n",
    "\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        pass"
   ],
   "id": "3c6e1f4d920bf6e0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础模型",
   "id": "e0f05df627ac3f48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.903171889Z",
     "start_time": "2026-01-19T12:12:54.746658846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(ABC):\n",
    "\n",
    "    def __init__(self, layer, loss_fn, optimizer):\n",
    "        self.layer = layer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, dataset, epochs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self, dataset):\n",
    "        pass"
   ],
   "id": "7d86c6176d4f3ce9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "4f5f451c02da72d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 数据集",
   "id": "b283d673396fdad0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.933667900Z",
     "start_time": "2026-01-19T12:12:54.910809045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def load(self):\n",
    "        self.train_features = [[22.5, 72.0],\n",
    "                               [31.4, 45.0],\n",
    "                               [19.8, 85.0],\n",
    "                               [27.6, 63]]\n",
    "        self.train_labels = [[95],\n",
    "                             [210],\n",
    "                             [70],\n",
    "                             [155]]\n",
    "\n",
    "        self.test_features = [[28.1, 58.0]]\n",
    "        self.test_labels = [[165]]"
   ],
   "id": "72e76757990e0348",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "1b39261f1bc9e096"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 线性层",
   "id": "424d1e2acf45088e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.967312530Z",
     "start_time": "2026-01-19T12:12:54.938070108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.weight = Tensor(np.random.rand(out_size, in_size) / in_size)\n",
    "        self.bias = Tensor(np.random.rand(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad += p.grad.T @ x.data\n",
    "            self.bias.grad += np.sum(p.grad, axis=0)\n",
    "            x.grad += p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear[weight{self.weight.data.shape}; bias{self.bias.data.shape}]'"
   ],
   "id": "92d9311acfda0ecf",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 顺序层",
   "id": "98b5cc515701a29c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:54.997142601Z",
     "start_time": "2026-01-19T12:12:54.971840944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join(str(l) for l in self.layers if str(l))"
   ],
   "id": "486a6b2b5dd13284",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ReLU激活函数\n",
    "\n",
    "ReLU激活函数是最常用的隐藏层激活函数。我们把它封装成一个单独的层。\n",
    "\n",
    "它的输出数据是一个张量：**激活值**（activation）。我们要提供激活值的梯度计算函数（gradient_fn）和父节点列表（parents）。\n",
    "\n",
    "梯度计算函数需要计算输入值的局部导数，并累加到输入值的梯度上。\n",
    "\n",
    "并且把输入值，也就是隐藏层输出的中间值，添加到父节点列表（parents）中。\n",
    "\n",
    "这里我们没有提供参数列表（parameters），因为激活函数没有权重这一类的模型参数需要通过反向传播来更新。\n",
    "\n",
    "---\n",
    "\n",
    "我们来看一下现在的计算图：\n",
    "\n",
    "```{figure} images/relu-compu-graph.png\n",
    ":align: center\n",
    ":width: 480px\n",
    "**图例：两层网络模型计算图结构**\n",
    "```\n",
    "\n",
    "* $x$：起点（特征值）\n",
    "* $parents$：父节点列表\n",
    "    * $h$：遍历节点（隐藏层中间值）\n",
    "    * $a$：遍历节点（激活值）\n",
    "    * $p$：遍历节点（预测值）\n",
    "* $parameters$：参数列表\n",
    "    * $w_1, b_1$：叶节点（隐藏层权重和偏置，不参与遍历）\n",
    "    * $w_2, b_2$：叶节点（输出层权重和偏置，不参与遍历）\n",
    "* $l$：终点（损失值）"
   ],
   "id": "1ae35fa4d77e31aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.022732956Z",
     "start_time": "2026-01-19T12:12:55.003247206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.maximum(0, x.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad += p.grad * (p.data > 0)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'ReLU[]'"
   ],
   "id": "89b5b279e1be9351",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 损失函数（均方误差）",
   "id": "564d9a52fd4e486c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.045333338Z",
     "start_time": "2026-01-19T12:12:55.026430665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSELoss(Loss):\n",
    "\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        mse = Tensor(np.mean(np.square(y.data - p.data)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad += -2 * (y.data - p.data) / len(y.data)\n",
    "\n",
    "        mse.gradient_fn = gradient_fn\n",
    "        mse.parents = {p}\n",
    "        return mse"
   ],
   "id": "f90c7271a461f86b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 优化器（随机梯度下降）",
   "id": "c4208a9fc5f51298"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.067906950Z",
     "start_time": "2026-01-19T12:12:55.049185858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDOptimizer(Optimizer):\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad * self.lr"
   ],
   "id": "2b78910fdfce6621",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 神经网络模型",
   "id": "8359292a154d7e03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.085915021Z",
     "start_time": "2026-01-19T12:12:55.071625753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NNModel(Model):\n",
    "\n",
    "    def train(self, dataset, epochs):\n",
    "        dataset.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(dataset)):\n",
    "                features, labels = dataset[i]\n",
    "\n",
    "                predictions = self.layer(features)\n",
    "                loss = self.loss_fn(predictions, labels)\n",
    "\n",
    "                self.optimizer.reset()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def test(self, dataset):\n",
    "        dataset.eval()\n",
    "\n",
    "        features, labels = dataset.items()\n",
    "        predictions = self.layer(features)\n",
    "        loss = self.loss_fn(predictions, labels)\n",
    "        return predictions, loss"
   ],
   "id": "bba9b83d7db71240",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 设置",
   "id": "a59488117759e301"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 学习率",
   "id": "725941d4fc83b308"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.100256697Z",
     "start_time": "2026-01-19T12:12:55.089214714Z"
    }
   },
   "cell_type": "code",
   "source": "LEARNING_RATE = 0.00001",
   "id": "adba47a241ed3e8e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 批大小",
   "id": "8bf1986dcc63604b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.115947052Z",
     "start_time": "2026-01-19T12:12:55.102915667Z"
    }
   },
   "cell_type": "code",
   "source": "BATCH_SIZE = 2",
   "id": "2595e442ad7428b8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 轮次",
   "id": "1a2c135127fda038"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.129609730Z",
     "start_time": "2026-01-19T12:12:55.119208189Z"
    }
   },
   "cell_type": "code",
   "source": "EPOCHS = 1000",
   "id": "f5770ada604b656",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "841997f8427fa7f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 建模\n",
    "\n",
    "我们可以通过简单地把激活函数层添加到线性层之后的方法，来完成一个完整的人工神经元的功能。"
   ],
   "id": "190dea01fd4172d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.171676022Z",
     "start_time": "2026-01-19T12:12:55.133166706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset(BATCH_SIZE)\n",
    "\n",
    "layer = Sequential([Linear(dataset.shape[0], 4),\n",
    "                    ReLU(),\n",
    "                    Linear(4, dataset.shape[1])])\n",
    "loss_fn = MSELoss()\n",
    "optimizer = SGDOptimizer(layer.parameters, lr=LEARNING_RATE)\n",
    "\n",
    "model = NNModel(layer, loss_fn, optimizer)\n",
    "print(model.layer)"
   ],
   "id": "4962e60cfdc0462c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear[weight(4, 2); bias(4,)]\n",
      "ReLU[]\n",
      "Linear[weight(1, 4); bias(1,)]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 迭代",
   "id": "68cdc2886a10c6da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.644551205Z",
     "start_time": "2026-01-19T12:12:55.175649148Z"
    }
   },
   "cell_type": "code",
   "source": "model.train(dataset, EPOCHS)",
   "id": "514175203a080205",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "8ed5febb1e791410"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 测试",
   "id": "245e3bf69f834167"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T12:12:55.704469034Z",
     "start_time": "2026-01-19T12:12:55.649549238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions, loss = model.test(dataset)\n",
    "print(f'prediction: {predictions}')\n",
    "print(f'loss: {loss}')"
   ],
   "id": "849d01453c043087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: Tensor([[164.86381116]])\n",
      "loss: Tensor(0.018547399366684963)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "到现在为止，我们的神经网络训练框架完整复现了小明的冰激凌销量预测模型。\n",
    "\n",
    "准确地讲，我们的神经网络训练框架已经可以很好地支持全连接网络模型，解决数值回归问题。"
   ],
   "id": "ef2fb1c2e64a0d05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
