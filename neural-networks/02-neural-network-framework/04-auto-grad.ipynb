{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 自动微分\n",
    "\n",
    "在梯度计算中，梯度是损失函数关于模型参数（权重、偏差）的偏导数。\n",
    "\n",
    "通常我们有两种直观的方法计算导数，但它们在深度学习面前都显得力不从心。\n",
    "\n",
    "1. 数值微分：\n",
    "\n",
    "我们可以通过数值模拟的方法，给每个参数增加一个微小的扰动$ϵ$来近似计算梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} \\approx \\frac{J(w_i + \\epsilon) - J(w_i)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "但这种方法在深度学习中是不可接受的。假设我们的网络有100万个参数，为了得到全部梯度，我们需要进行100万次前向传播计算（即运行100万次模型推理），这会造成灾难性的计算开销。\n",
    "\n",
    "2. 符号微分：\n",
    "\n",
    "另一种方法，是像我们在第一部分关于多层神经元网络的章节里做的那样，手动推导出损失函数的数学解析式。\n",
    "\n",
    "但是对于简单的线性回归，手动推导很容易。如果我们有100层网络，包含各种更复杂的变换，手动推导出的导数公式将长达数页。而一旦修改了网络结构（比如加个层），所有公式都要推倒重来，极其缺乏灵活性。\n",
    "\n",
    "在深度学习的实践中，普遍采用的是第三种方法：\n",
    "\n",
    "3. 自动微分：\n",
    "\n",
    "**自动微分**（Automatic Differentiation）的实现依据是微积分的**链式法则**（Chain Rule）；它的设计思路是：任何复杂的运算，在计算机底层都是由一系列基本运算（如加、减、乘、除、指数、对数）组合而成的。\n",
    "\n",
    "* 分解计算图：我们将复杂的公式拆解为一个计算图（Computational Graph）。每一个节点只负责一个极简单的基本运算。\n",
    "* 局部求导：对于每个基本运算（如$y=a⋅b$），我们预先定义好它的求导规则。\n",
    "* 根据微积分的链式法则：\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "当我们从损失函数开始梯度计算时，模型并不需要知道所有导数的完整复杂公式，它只需要从损失值开始倒着走：\n",
    "* 拿到当前层对下一层的梯度（上游梯度）。\n",
    "* 乘上当前算子的局部导数（Local Gradient）。\n",
    "* 结果传回给上一层节点。\n",
    "\n",
    "通过这种方式，我们只需要执行一次前向传播（记录路径）和一次梯度计算（应用链式法则），就能以极高的精度计算出所有参数的梯度。\n"
   ],
   "id": "c0dcf040878af3b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:02.975411267Z",
     "start_time": "2026-01-05T21:52:02.956120270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import numpy as np"
   ],
   "id": "f2f5f22af2a86f3e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基础架构",
   "id": "fb5095cca95df5ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 张量\n",
    "\n",
    "张量不仅是保存数据的地方，也是进行梯度计算，和计算链传播的地方。为此，我们增加了三个变量：\n",
    "\n",
    "* grad：保存梯度\n",
    "* gradient_fn：局部导数计算函数\n",
    "* parents：上一层节点（构成梯度计算链）\n",
    "\n",
    "同时，我们添加了一个函数：\n",
    "* backward()：调用gradient_fn计算局部梯度，并调用parents中所有上一层节点进行梯度计算。这是一个递归调用过程，将触发所有上级节点，完成全部梯度计算。"
   ],
   "id": "3dcda0116d9179b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.078971151Z",
     "start_time": "2026-01-05T21:52:03.017318847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.backward()\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data)"
   ],
   "id": "c73ebea589dd19d8",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础层",
   "id": "3e4acfae657d6646"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.199026556Z",
     "start_time": "2026-01-05T21:52:03.124350856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.forward(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return ''"
   ],
   "id": "da65a5787200d264",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础损失函数",
   "id": "84802851c1fc1134"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.222684558Z",
     "start_time": "2026-01-05T21:52:03.204633209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(ABC):\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        return self.loss(p, y)\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        pass"
   ],
   "id": "69b74596c9d28e1",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "13f0e80884d80347"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征、标签",
   "id": "622461c44697b9a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.258215317Z",
     "start_time": "2026-01-05T21:52:03.226672532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = Tensor([28.1, 58.0])\n",
    "label = Tensor([165])"
   ],
   "id": "14f53a4928b9df76",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "8ffc9de057b4edf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性层\n",
    "\n",
    "在线性层，我们要在**预测值张量**中添加线性回归的梯度计算函数，并将结果分别保存在**权重**和**偏差**的梯度变量（grad）中。"
   ],
   "id": "545906e4085aba16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.291097890Z",
     "start_time": "2026-01-05T21:52:03.264902290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.weight = Tensor(np.ones((out_size, in_size)) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad += p.grad * x.data\n",
    "            self.bias.grad += np.sum(p.grad, axis=0)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        return p\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'weight: {self.weight}\\nbias: {self.bias}'"
   ],
   "id": "c593414181439508",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 损失函数（平均平方差）\n",
    "\n",
    "损失函数是梯度计算的起点。所以我们也需要在**损失值张量**中添加平均平方差的梯度计算函数，并将结果保存在**预测值**的梯度变量（grad）中。\n",
    "\n",
    "同时，要将预测值添加到上一级节点的列表中。"
   ],
   "id": "4c73a7f93691f370"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.326997008Z",
     "start_time": "2026-01-05T21:52:03.294017397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSELoss(Loss):\n",
    "\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        mse = Tensor(np.mean(np.square(y.data - p.data)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad += -2 * (y.data - p.data)\n",
    "\n",
    "        mse.gradient_fn = gradient_fn\n",
    "        mse.parents = {p}\n",
    "        return mse"
   ],
   "id": "a0a203ff2812af61",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "e83721e80337da30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理",
   "id": "245e3bf69f834167"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.491264328Z",
     "start_time": "2026-01-05T21:52:03.336545246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer = Linear(feature.size(), 1)\n",
    "prediction = layer(feature)\n",
    "print(f'prediction: {prediction}')"
   ],
   "id": "849d01453c043087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [43.05]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估",
   "id": "12197a90e7728685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.760455445Z",
     "start_time": "2026-01-05T21:52:03.537949191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = MSELoss()\n",
    "error = loss(prediction, label)\n",
    "print(f'error: {error}')"
   ],
   "id": "89567b98057d0120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 14871.802500000002\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "74d2f2d591d8e756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 梯度计算\n",
    "\n",
    "现在，我们在梯度计算的时候，只需要调用损失值张量中的梯度计算函数。它将完成预测值梯度的计算，并传递到预测值张量完成权重和偏差的梯度计算。"
   ],
   "id": "16a299a9855ae300"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.822567270Z",
     "start_time": "2026-01-05T21:52:03.790347858Z"
    }
   },
   "cell_type": "code",
   "source": "error.backward()",
   "id": "218bd08752c1b980",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向传播\n",
    "\n",
    "在反向传播的步骤，我们需要让每个张量减去它的的梯度值，就完成了一次迭代。"
   ],
   "id": "e230a82951ce9b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:03.847187761Z",
     "start_time": "2026-01-05T21:52:03.826466088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer.weight.data -= layer.weight.grad\n",
    "layer.bias.data -= layer.bias.grad\n",
    "print(layer)"
   ],
   "id": "5508d161fbd05771",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [[ 6854.09 14146.7 ]]\n",
      "bias: [243.9]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 重新评估",
   "id": "807037d89ea24a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:04.034134815Z",
     "start_time": "2026-01-05T21:52:03.890905925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = layer(feature)\n",
    "error = loss(prediction, label)\n",
    "print(f'error: {error}')"
   ],
   "id": "4b236ea915e49801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1026548766283.6302\n"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
