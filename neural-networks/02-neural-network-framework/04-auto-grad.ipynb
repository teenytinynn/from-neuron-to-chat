{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 自动微分\n",
    "\n",
    "在梯度计算中，梯度是损失函数关于模型参数的导数。通常我们有两种直观的方法计算导数，但它们在深度学习面前都显得力不从心：\n",
    "\n",
    "1. **数值微分**：\n",
    "\n",
    "我们可以通过数值模拟的方法，给每个参数增加一个微小的扰动 $ϵ$ 来近似计算梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} \\approx \\frac{J(w_i + \\epsilon) - J(w_i)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "但这种方法在深度学习中是不可接受的。假设我们的网络模型有 100 万个参数，为了得到全部梯度，我们需要进行 100 万次前向传播计算（运行 100 万次模型推理），这会造成灾难性的计算开销。\n",
    "\n",
    "2. **符号微分**：\n",
    "\n",
    "另一种方法，是像我们在第一部分里做的那样，手动推导出损失函数的数学解析式。\n",
    "\n",
    "对于简单的线性回归，手动推导很容易。但是如果我们有 100 层网络，且包含各种更复杂的变换，手动推导出的导数公式可能就有数页。而一旦我们修改了网络结构（比如加个层），所有公式又要推倒重来，极其缺乏灵活性。\n",
    "\n",
    "在深度学习的实践中，普遍采用的是第三种方法：\n",
    "\n",
    "3. **自动微分**：\n",
    "\n",
    "**自动微分**（Automatic Differentiation）的实现依据是微积分的**链式法则**（Chain Rule）；它的设计思路是：任何复杂的运算，在计算机底层都是由一系列基本运算（如加、减、乘、除、指数、对数）组合而成的，所以我们可以:\n",
    "\n",
    "* 分解计算图：我们将复杂的公式拆解为一个计算图。每一个节点只负责一个简单的基本运算；\n",
    "* 局部求导：对于每个基本运算（如 $p=w \\cdot x + b$ ），我们可以预先定义好它的求导规则；\n",
    "* 根据微积分的链式法则：\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "当我们从损失函数开始梯度计算时，模型并不需要知道所有导数的完整复杂公式，它只需要从损失值开始倒着走：\n",
    "* 拿到后一层传来的上游梯度；\n",
    "* 乘上当前层的局部导数；\n",
    "* 结果继续传给父节点（前一层节点）。\n",
    "\n",
    "通过这种方式，我们只需要执行一次前向传播（记录计算图），和一次梯度计算（应用链式法则），就能计算出所有参数的梯度。\n"
   ],
   "id": "c0dcf040878af3b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.044489690Z",
     "start_time": "2026-01-18T14:01:50.019584922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import numpy as np"
   ],
   "id": "f2f5f22af2a86f3e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基础架构",
   "id": "fb5095cca95df5ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 张量\n",
    "\n",
    "计算图中的所有节点都是张量。\n",
    "\n",
    "张量不仅是保存数据的地方，也是计算梯度的地方，是梯度计算链路传播的地方。为此，我们增加了三个属性：\n",
    "\n",
    "* **梯度**（grad）：保存本张量的梯度\n",
    "* **梯度计算函数**（gradient_fn）：计算局部导数\n",
    "* **父节点**（parents）：构成梯度计算链路\n",
    "\n",
    "同时，我们添加了一个函数：\n",
    "* **反向函数**（backward）：调用梯度计算函数计算局部导数，并调用父节点中所有前一层节点进行梯度计算。这是一个递归调用过程，将触发所有上级节点，完成全部梯度计算。"
   ],
   "id": "3dcda0116d9179b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.068874982Z",
     "start_time": "2026-01-18T14:01:50.053630250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def backward(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.backward()\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.data})'"
   ],
   "id": "c73ebea589dd19d8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础层",
   "id": "3e4acfae657d6646"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.095566354Z",
     "start_time": "2026-01-18T14:01:50.072505068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __call__(self, x: Tensor):\n",
    "        return self.forward(x)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: Tensor):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ''"
   ],
   "id": "da65a5787200d264",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 基础损失函数",
   "id": "84802851c1fc1134"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.189868269Z",
     "start_time": "2026-01-18T14:01:50.101984619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss(ABC):\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        return self.loss(p, y)\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        pass"
   ],
   "id": "69b74596c9d28e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "13f0e80884d80347"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征、标签",
   "id": "622461c44697b9a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.213852581Z",
     "start_time": "2026-01-18T14:01:50.196215058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = Tensor([28.1, 58.0])\n",
    "label = Tensor([165])"
   ],
   "id": "14f53a4928b9df76",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "8ffc9de057b4edf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性层\n",
    "\n",
    "在线性层，我们要在**预测值张量**中添加线性回归的梯度计算函数，并将结果分别保存在**权重张量**和**偏置张量**的梯度（grad）中。"
   ],
   "id": "545906e4085aba16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.238323378Z",
     "start_time": "2026-01-18T14:01:50.219594796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.weight = Tensor(np.ones((out_size, in_size)) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad += p.grad * x.data\n",
    "            self.bias.grad += np.sum(p.grad, axis=0)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        return p\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Linear[weight{self.weight.data.shape}; bias{self.bias.data.shape}]'"
   ],
   "id": "c593414181439508",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 损失函数（均方误差）\n",
    "\n",
    "损失函数是梯度计算的起点。所以我们也需要在**损失值张量**中添加均方误差的梯度计算函数，并将结果保存在**预测值张量**的梯度（grad）中。\n",
    "\n",
    "同时，要将预测值添加到父节点列表中，成为计算图的一部分。"
   ],
   "id": "4c73a7f93691f370"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.264929844Z",
     "start_time": "2026-01-18T14:01:50.241431014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSELoss(Loss):\n",
    "\n",
    "    def loss(self, p: Tensor, y: Tensor):\n",
    "        mse = Tensor(np.mean(np.square(y.data - p.data)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad += -2 * (y.data - p.data)\n",
    "\n",
    "        mse.gradient_fn = gradient_fn\n",
    "        mse.parents = {p}\n",
    "        return mse"
   ],
   "id": "a0a203ff2812af61",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "e83721e80337da30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 建模",
   "id": "94adc1621a9a0454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.333762306Z",
     "start_time": "2026-01-18T14:01:50.272932092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer = Linear(feature.size, 1)\n",
    "print(layer)"
   ],
   "id": "613843e83535be75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear[weight(1, 2); bias(1,)]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理",
   "id": "245e3bf69f834167"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.411892644Z",
     "start_time": "2026-01-18T14:01:50.339442400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = layer(feature)\n",
    "print(f'prediction: {prediction}')"
   ],
   "id": "849d01453c043087",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: Tensor([43.05])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们在开始模型推理的同时，开始构建动态计算图。",
   "id": "a270e77148fbcb0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估",
   "id": "12197a90e7728685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.490745652Z",
     "start_time": "2026-01-18T14:01:50.416295596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = MSELoss()\n",
    "loss = loss_fn(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "89567b98057d0120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: Tensor(14871.802500000002)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "我们在评估模型的同时，也完成动态计算图的构建。\n",
    "\n",
    "特征值是计算图的起点，损失值是计算图的终点。计算图的遍历节点目前只有一个，就是预测值。权重和偏置算是叶节点，不参与遍历。\n",
    "\n",
    "```{figure} images/compu-graph.png\n",
    ":align: center\n",
    ":width: 300px\n",
    "**图例：计算图结构**\n",
    "```\n",
    "\n",
    "* $x$：起点（特征值）\n",
    "* $p$：遍历节点（预测值）\n",
    "* $w, b$：叶节点（权重和偏置，不参与遍历）\n",
    "* $l$：终点（损失值）"
   ],
   "id": "dd2091683b9dd1e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "74d2f2d591d8e756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 梯度计算\n",
    "\n",
    "现在，我们在梯度计算的时候，只需要调用**损失值张量**中的反向函数。它将：\n",
    "* 计算损失值的梯度；\n",
    "* 通过父节点列表，遍历计算图；\n",
    "* 计算所有节点的梯度。"
   ],
   "id": "16a299a9855ae300"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.531029623Z",
     "start_time": "2026-01-18T14:01:50.507128695Z"
    }
   },
   "cell_type": "code",
   "source": "loss.backward()",
   "id": "218bd08752c1b980",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向传播\n",
    "\n",
    "在反向传播的步骤，我们需要让每个参数张量减去它的的梯度值，就完成了一次迭代。"
   ],
   "id": "e230a82951ce9b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.557439898Z",
     "start_time": "2026-01-18T14:01:50.539864647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer.weight.data -= layer.weight.grad\n",
    "layer.bias.data -= layer.bias.grad"
   ],
   "id": "5508d161fbd05771",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 重新评估",
   "id": "807037d89ea24a4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T14:01:50.665419278Z",
     "start_time": "2026-01-18T14:01:50.561282678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = layer(feature)\n",
    "loss = loss_fn(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "4b236ea915e49801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: Tensor(1026548766283.6302)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们升级了张量类，通过梯度计算函数实现了自动微分；通过父节点实现了动态计算图。至此，我们实现了梯度计算链路的自动化。",
   "id": "fec4755598fed61e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
