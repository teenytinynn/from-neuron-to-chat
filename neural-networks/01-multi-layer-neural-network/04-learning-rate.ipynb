{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 学习率\n",
    "\n",
    "在上一个章节，我们尝试使用梯度下降法进行模型训练。结果一顿操作猛如虎，**损失值不仅没有减小，反而急剧增大**！\n",
    "\n",
    "问题出在哪里？\n",
    "\n",
    "答案是：**更新的步子迈得太大了！**\n",
    "\n",
    "---\n",
    "\n",
    "梯度的本质是损失函数关于模型参数的导数，是损失函数曲线在某一点的切线斜率。它是下降的方向，并不是下降的幅度。\n",
    "\n",
    "梯度很大，只代表道路很陡峭，不代表步子应该迈很大。\n",
    "\n",
    "更新的步长太大，可能直接越过函数的最低点。后果就是损失值在最低点的左右震荡，无法收敛，最终导致模型训练失败。在深度学习中，这种现象被称为**发散**（Divergence）。\n",
    "\n",
    "---\n",
    "\n",
    "为了控制更新的步长，我们引入一个比例系数，称为**学习率**（Learning Rate），记作 $\\eta$。这样梯度下降的公式更新为：\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "通过调整学习率，我们可以控制模型训练的步幅。既不应太小，造成损失函数收敛太慢；也不能太大，造成损失函数左右震荡、甚至无法收敛。\n",
    "\n",
    "## 超参数\n",
    "\n",
    "学习率是我们遇到的第一个**超参数**（Hyperparameter）。\n",
    "\n",
    "我们已经知道，模型参数包括权重和偏置，是模型训练的对象；而超参数是我们控制模型训练幅度的参数，比如学习率是我们控制梯度下降步长的参数。\n",
    "\n",
    "**超参数不会被网络模型学习**。而是需要在模型训练开始前，根据数据、经验等实际情况手动设定，并根据模型训练的效果进行调整，以期获得最佳训练结果。这个不断测试、调整超参数的过程，被称为**调参**（Hyperparameter Tuning）。"
   ],
   "id": "8b1718296e14f742"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:49.903748096Z",
     "start_time": "2026-01-16T19:28:49.872963593Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "c790c2847a5addf8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "239f4cdf3b19b187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征、标签",
   "id": "ed890d7484ee20a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:49.965455781Z",
     "start_time": "2026-01-16T19:28:49.912758306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = np.array([28.1, 58.0])\n",
    "label = np.array([165.0])"
   ],
   "id": "6c1dd66a30855524",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "9d51956d4698a34e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 参数：权重、偏置",
   "id": "d537395bac936f2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.067021720Z",
     "start_time": "2026-01-16T19:28:49.994730547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = np.ones(2) / 2\n",
    "bias = np.zeros(1)"
   ],
   "id": "81298db1421a07bd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理函数",
   "id": "e2b9dc3ebbffe209"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.378593734Z",
     "start_time": "2026-01-16T19:28:50.128992465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward(x, w, b):\n",
    "    return x @ w.T + b"
   ],
   "id": "659e728278131e0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 损失函数（均方误差）",
   "id": "91ba7f0c0077c7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.410288023Z",
     "start_time": "2026-01-16T19:28:50.384914332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mse_loss(p, y):\n",
    "    return np.mean(np.square(y - p))"
   ],
   "id": "33403dc359850248",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度函数",
   "id": "f0f1a1e8e62cb1f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.441374882Z",
     "start_time": "2026-01-16T19:28:50.415574986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient(p, y):\n",
    "    return - 2 * (y - p)"
   ],
   "id": "6159e0137e7326b3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向函数\n",
    "\n",
    "改进后的反向函数利用学习率来调整模型训练的幅度。"
   ],
   "id": "782e08c9239c7fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.496193909Z",
     "start_time": "2026-01-16T19:28:50.447392165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward(x, d, w, b, lr):\n",
    "    w = w - d * x * lr\n",
    "    b = b - d * lr\n",
    "    return w, b"
   ],
   "id": "18301d372b2cdaf1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "e38110a19341a207"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 超参数：学习率\n",
    "\n",
    "学习率的选择依赖经验与实验。常见策略是从一个典型值（比如：0.1、0.01、0.001）开始，观察训练效果，逐步调整。\n",
    "\n",
    "在我们的例子中，第一次模型训练得到的梯度值非常大。因此我们选择了一个较小的学习率:"
   ],
   "id": "a3e5cb6e8d818a73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.608622870Z",
     "start_time": "2026-01-16T19:28:50.508948125Z"
    }
   },
   "cell_type": "code",
   "source": "LEARNING_RATE = 0.00001",
   "id": "1cac82de22c93e0f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理",
   "id": "1f009cb73f3b359b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.659203239Z",
     "start_time": "2026-01-16T19:28:50.617565907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "print(f'prediction: {prediction}')"
   ],
   "id": "80233f0e967e1603",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [43.05]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估",
   "id": "1e93b833d4a219ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.769513804Z",
     "start_time": "2026-01-16T19:28:50.730698190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "608008ce09c030aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14871.802500000002\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "ece16e12446f6a95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度计算",
   "id": "d28ead0eb62e1635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.891080303Z",
     "start_time": "2026-01-16T19:28:50.849182402Z"
    }
   },
   "cell_type": "code",
   "source": "delta = gradient(prediction, label)",
   "id": "47b16b7320dd401d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向传播\n",
    "\n",
    "反向传播的过程中，我们利用学习率来调整权重和偏置的更新幅度。"
   ],
   "id": "9bf4b2cb78a2301"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:50.992521399Z",
     "start_time": "2026-01-16T19:28:50.899694581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight, bias = backward(feature, delta, weight, bias, LEARNING_RATE)\n",
    "print(f\"weight: {weight}\")\n",
    "print(f\"bias: {bias}\")"
   ],
   "id": "1afc07eb317ef3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [0.5685359 0.641462 ]\n",
      "bias: [0.002439]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "可以看出，通过引入合适的学习率，经过一次模型训练，权重和偏置的更新幅度变得相对合理。实际结果如何呢？",
   "id": "2c6c4b0c721865b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 重新评估",
   "id": "3b8b6a2d5b84392a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T19:28:51.331832004Z",
     "start_time": "2026-01-16T19:28:51.004917729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "cd20ce9cb9a2e8db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12503.020514375934\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过重新评估可以看出：通过引入学习率，第一次模型训练使损失值下降了大约20%。这已经在一个正常、合理的范围了。",
   "id": "2ce0d17887f6c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 课后练习\n",
    "\n",
    "尝试不同的学习率（如 0.001, 0.0001, 0.000001），观察：\n",
    "\n",
    "* 权重和偏置的更新幅度；\n",
    "* 损失值的变化趋势；\n",
    "* 是否出现震荡或无变化的现象。"
   ],
   "id": "ea8258c477d519a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
