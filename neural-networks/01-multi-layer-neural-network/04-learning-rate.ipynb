{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 学习率\n",
    "\n",
    "在上一个章节，我们尝试使用梯度下降进行模型训练。结果一顿操作猛如虎，**损失值不仅没有减小，反而急剧增大**！\n",
    "\n",
    "问题出在哪里？\n",
    "\n",
    "答案是：**更新的步子迈得太大了！**\n",
    "\n",
    "---\n",
    "\n",
    "梯度的本质是损失函数对模型参数的导数，是损失函数曲线在某一点的切线斜率。它是下降的方向，并不是下降的幅度。\n",
    "\n",
    "梯度很大，只代表道路很陡峭，不代表步子应该迈很大。\n",
    "\n",
    "更新的步长太大，可能直接越过函数的最低点。后果就是损失值在最低点的左右震荡，无法收敛，最终导致模型训练失败。在深度学习中，这种现象被称为**发散**（Divergence）。\n",
    "\n",
    "---\n",
    "\n",
    "为控制更新的步长，我们引入一个比例系数，称为**学习率**（Learning Rate），记作 $\\eta$。这样梯度下降的公式更新为：\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "通过调整学习率，我们可以控制模型训练的步幅。既不应太小，造成损失函数收敛太慢；也不能太大，造成损失函数左右震荡、甚至无法收敛。\n",
    "\n",
    "## 超参数\n",
    "\n",
    "学习率是我们遇到的第一个**超参数**（Hyperparameter）。\n",
    "\n",
    "我们已经知道模型参数包括权重和偏置。模型参数是模型训练的对象；而超参数是我们控制模型训练幅度的参数，比如学习率是我们控制梯度下降步长的参数。\n",
    "\n",
    "**超参数不会被模型学习**。而是需要在模型训练开始前，根据数据、经验等实际情况手动设定，并根据模型训练的效果进行调整，以期获得最佳训练结果。这个不断测试、调整超参数的过程，被称为**调参**（Hyperparameter Tuning）。"
   ],
   "id": "8b1718296e14f742"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.408057920Z",
     "start_time": "2026-01-05T21:52:09.392388817Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "c790c2847a5addf8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "239f4cdf3b19b187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征、标签",
   "id": "ed890d7484ee20a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.425550914Z",
     "start_time": "2026-01-05T21:52:09.413447217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = np.array([28.1, 58.0])\n",
    "label = np.array([165.0])"
   ],
   "id": "6c1dd66a30855524",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "9d51956d4698a34e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 参数：权重、偏置",
   "id": "d537395bac936f2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.445407808Z",
     "start_time": "2026-01-05T21:52:09.428948249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = np.ones([1, 2]) / 2\n",
    "bias = np.zeros(1)"
   ],
   "id": "81298db1421a07bd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理函数",
   "id": "e2b9dc3ebbffe209"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.462521536Z",
     "start_time": "2026-01-05T21:52:09.448807105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward(x, w, b):\n",
    "    return x @ w.T + b"
   ],
   "id": "659e728278131e0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 损失函数（平均平方差）",
   "id": "91ba7f0c0077c7b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.476885334Z",
     "start_time": "2026-01-05T21:52:09.464901219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mse_loss(p, y):\n",
    "    return np.mean(np.square(y - p))"
   ],
   "id": "33403dc359850248",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度函数",
   "id": "f0f1a1e8e62cb1f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.494185810Z",
     "start_time": "2026-01-05T21:52:09.480759430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient(p, y):\n",
    "    return - 2 * (y - p)"
   ],
   "id": "6159e0137e7326b3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向函数\n",
    "\n",
    "改进后的反向函数利用学习率来调整模型训练的幅度。"
   ],
   "id": "782e08c9239c7fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.514822936Z",
     "start_time": "2026-01-05T21:52:09.499424245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward(x, d, w, b, lr):\n",
    "    w = w - d * x * lr\n",
    "    b = b - d * lr\n",
    "    return w, b"
   ],
   "id": "18301d372b2cdaf1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "e38110a19341a207"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 超参数：学习率\n",
    "\n",
    "学习率的选择依赖经验与实验。常见策略是从一个典型值（比如：0.1、0.01、0.001）开始，观察训练行为，逐步调整。\n",
    "\n",
    "在我们的例子中，第一次模型训练得到的梯度值非常大。因此我们选择了一个较小的学习率:"
   ],
   "id": "a3e5cb6e8d818a73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.528058406Z",
     "start_time": "2026-01-05T21:52:09.516520951Z"
    }
   },
   "cell_type": "code",
   "source": "LEARNING_RATE = 0.00001",
   "id": "1cac82de22c93e0f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理",
   "id": "1f009cb73f3b359b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.598312379Z",
     "start_time": "2026-01-05T21:52:09.531079389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "print(f'prediction: {prediction}')"
   ],
   "id": "80233f0e967e1603",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [43.05]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估",
   "id": "1e93b833d4a219ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.704755180Z",
     "start_time": "2026-01-05T21:52:09.609715075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "608008ce09c030aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14871.802500000002\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练",
   "id": "ece16e12446f6a95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度计算",
   "id": "d28ead0eb62e1635"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.726748261Z",
     "start_time": "2026-01-05T21:52:09.709079453Z"
    }
   },
   "cell_type": "code",
   "source": "delta = gradient(prediction, label)",
   "id": "47b16b7320dd401d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 反向传播",
   "id": "9bf4b2cb78a2301"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.791050560Z",
     "start_time": "2026-01-05T21:52:09.730648538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight, bias = backward(feature, delta, weight, bias, LEARNING_RATE)\n",
    "print(f\"weight: {weight}\")\n",
    "print(f\"bias: {bias}\")"
   ],
   "id": "1afc07eb317ef3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [[0.5685359 0.641462 ]]\n",
      "bias: [0.002439]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "可以看出，通过引入合适的学习率，经过一次模型训练，权重和偏置的调整幅度变得相对合理。实际结果如何呢？",
   "id": "2c6c4b0c721865b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 重新评估",
   "id": "3b8b6a2d5b84392a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-05T21:52:09.842935596Z",
     "start_time": "2026-01-05T21:52:09.794585891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "cd20ce9cb9a2e8db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12503.020514375934\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "通过重新评估可以看出：通过引入学习率，第一次模型训练使损失值下降了大约20%。这已经在一个正常、合理的范围了。",
   "id": "2ce0d17887f6c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 课后练习\n",
    "\n",
    "尝试不同的学习率（如 0.001, 0.0001, 0.000001），观察：\n",
    "\n",
    "* 权重和偏置的更新幅度；\n",
    "* 损失值的变化趋势；\n",
    "* 是否出现震荡或无变化的现象。"
   ],
   "id": "ea8258c477d519a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
