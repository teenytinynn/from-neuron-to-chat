{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# 梯度下降\n",
    "\n",
    "到目前为止，我们已经构建了一个最简单的神经网络模型的基本框架：\n",
    "\n",
    "* 模型参数：权重和偏置；\n",
    "* 模型逻辑：前向传播和损失计算。\n",
    "\n",
    "其中，模型逻辑是固定的，**网络模型的准确性取决于模型参数是否合理**。\n",
    "\n",
    "那么，如何获得合理的参数？\n",
    "\n",
    "答案是：让网络模型从数据中学习，这个过程称为**模型训练**（Model Training）。\n",
    "\n",
    "## 模型训练\n",
    "\n",
    "模型训练的目标很明确：**让损失函数的输出（损失值）尽可能小**。\n",
    "\n",
    "神经元模型的本质是线性回归。我们可以对损失函数使用微积分的**梯度下降法**（Gradient Descent），来寻找更优的参数。\n",
    "\n",
    "---\n",
    "\n",
    "如果我们把推理函数代入损失函数，就会发现**损失函数是模型参数的函数**。可以表示为 $L(w, b)$。\n",
    "\n",
    "在梯度下降法中，**梯度**（Gradient）是损失函数在某一点增加最快的方向，实质就是损失函数的导数。可以把梯度表示为：\n",
    "\n",
    "$$\n",
    "\\nabla L = \\left( \\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b} \\right)\n",
    "$$\n",
    "\n",
    "**下降**（Descent）就是让模型参数沿着梯度相反的方向变化，从而达到让损失值变小的目的：\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\frac{\\partial L}{\\partial b}\n",
    "$$"
   ],
   "id": "858e685ef64b01ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.683631Z",
     "start_time": "2026-01-25T20:50:13.680847Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "98f522323c77cd1f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据",
   "id": "ed262c084ffeff2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 特征、标签",
   "id": "a986148645ffbe00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.686680Z",
     "start_time": "2026-01-25T20:50:13.683932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature = np.array([28.1, 58.0])\n",
    "label = np.array([165])"
   ],
   "id": "5dc9076f8838ff0f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型",
   "id": "e0f988794356586d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 参数：权重、偏置",
   "id": "b80c9aa0309b7bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.689621Z",
     "start_time": "2026-01-25T20:50:13.686927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = np.ones((1, 2)) / 2\n",
    "bias = np.zeros(1)"
   ],
   "id": "37aee2fbe9625d76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理函数",
   "id": "584cb717f5f19d2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.692790Z",
     "start_time": "2026-01-25T20:50:13.689847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward(x, w, b):\n",
    "    return x @ w.T + b"
   ],
   "id": "c9a5523d452aa003",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 损失函数（均方误差）",
   "id": "e898e53991ecde17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.695774Z",
     "start_time": "2026-01-25T20:50:13.693036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mse_loss(p, y):\n",
    "    return np.mean(np.square(y - p))"
   ],
   "id": "20e825f7e40efb91",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 梯度函数\n",
    "\n",
    "损失值是预测值的函数（损失函数）；预测值是模型参数的函数（推理函数）。\n",
    "\n",
    "根据微积分的**链式规则**（Chain Rule），我们可以分别计算两个函数的导数，然后将其相乘，从而获得损失值关于模型参数的导数。\n",
    "\n",
    "---\n",
    "\n",
    "损失函数的公式是：$L = (y - p)^2$，导数公式是：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p} = -2(y - p)\n",
    "$$\n",
    "\n",
    "推理函数的公式是：$p = w \\cdot x + b$，权重和偏置的导数公式分别是：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial w} = x\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "因此，根据链式规则，权重和偏置的梯度分别是：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial p} \\cdot \\frac{\\partial p}{\\partial w} = -2(y - p) \\cdot x\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial p} \\cdot \\frac{\\partial p}{\\partial b} = -2(y - p)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "梯度函数将计算权重梯度和偏置梯度的共同部分：$-2(y - p)$。\n",
    "\n",
    "在深度学习中，这个共同部分通常被称为**误差项**（Delta）。它是从损失函数反向传播的第一棒：\n",
    "\n",
    "$$\n",
    "\\delta = -2(y - p)\n",
    "$$"
   ],
   "id": "9cff97645d4a7fd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.701063Z",
     "start_time": "2026-01-25T20:50:13.698071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient(p, y):\n",
    "    return - 2 * (y - p)"
   ],
   "id": "ff83d27fd0b1633b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向函数\n",
    "\n",
    "反向函数将根据梯度调整权重和偏置。这种数据从输出向输入方向的流动，被称为**反向传播**（Backpropagation）：\n",
    "\n",
    "$$\n",
    "w = w - \\delta \\cdot x\n",
    "$$\n",
    "$$\n",
    "b = b - \\delta \\cdot 1\n",
    "$$"
   ],
   "id": "c12ee2534aa2c0f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.703922Z",
     "start_time": "2026-01-25T20:50:13.701303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward(x, d, w, b):\n",
    "    w -= d * x\n",
    "    b -= d\n",
    "    return w, b"
   ],
   "id": "15c244f1210c0188",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证",
   "id": "fd69af83ac69912f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 推理",
   "id": "970311079abb6db3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.711526Z",
     "start_time": "2026-01-25T20:50:13.704148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "print(f'prediction: {prediction}')"
   ],
   "id": "11237f48834e66dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [43.05]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 评估",
   "id": "fb70fe9d5b31f624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.720218Z",
     "start_time": "2026-01-25T20:50:13.711942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "7c057d4ef0367bd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14871.802500000002\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 训练\n",
    "\n",
    "现在，我们根据梯度下降的理论来进行第一次的模型训练。"
   ],
   "id": "15370c3b8e446497"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度计算",
   "id": "2d543fbe9eefa2d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.742599Z",
     "start_time": "2026-01-25T20:50:13.720800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "delta = gradient(prediction, label)\n",
    "print(f'delta: {delta}')"
   ],
   "id": "8d045d95e9dc24a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta: [-243.9]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 反向传播",
   "id": "6acca0570f0b4714"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.755058Z",
     "start_time": "2026-01-25T20:50:13.743820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight, bias = backward(feature, delta, weight, bias)\n",
    "print(f\"weight: {weight}\")\n",
    "print(f\"bias: {bias}\")"
   ],
   "id": "5043693c2ea19d4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [[ 6854.09 14146.7 ]]\n",
      "bias: [243.9]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "经过一次模型训练，权重和偏置都有了明显的变化。效果如何呢？",
   "id": "482c49c4f109651e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 重新评估",
   "id": "fc22697d6edc3db8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T20:50:13.768390Z",
     "start_time": "2026-01-25T20:50:13.757690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction = forward(feature, weight, bias)\n",
    "loss = mse_loss(prediction, label)\n",
    "print(f'loss: {loss}')"
   ],
   "id": "b44ff15e18970cb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1026548766283.6302\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "不幸的是，损失值严重恶化。这又是为什么呢？",
   "id": "31268071a2e85913"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 课后练习\n",
    "\n",
    "思考一下，什么原因造成了损失值的恶化？有什么办法可以解决这个问题？"
   ],
   "id": "71b6fe5f4a5a1d52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
